{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9391e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1cb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb8a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/ashle/Springboard/Spark SQL/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75851a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df2_path = data_path + '/utilization.json'\n",
    "df2 = spark.read.format('json').load(json_df2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "612d5268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65cb88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce2e030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpu_utilization',\n",
       " 'event_datetime',\n",
       " 'free_memory',\n",
       " 'server_id',\n",
       " 'session_count']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ddbd5",
   "metadata": {},
   "source": [
    "**Basic Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae84c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_sample = df2.sample(False, fraction = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f604ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|\n",
      "|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|\n",
      "|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|\n",
      "|           0.62|03/05/2019 10:36:14|       0.36|      100|           50|\n",
      "|            0.5|03/05/2019 12:16:14|        0.5|      100|           51|\n",
      "|           0.65|03/05/2019 12:21:14|       0.37|      100|           65|\n",
      "|           0.29|03/05/2019 13:11:14|       0.48|      100|           48|\n",
      "|           0.39|03/05/2019 14:46:14|       0.43|      100|           39|\n",
      "|           0.63|03/05/2019 15:56:14|       0.42|      100|           61|\n",
      "|           0.39|03/05/2019 16:01:14|       0.34|      100|           60|\n",
      "|           0.58|03/05/2019 16:56:14|       0.38|      100|           67|\n",
      "|           0.57|03/05/2019 19:56:14|       0.36|      100|           43|\n",
      "|           0.32|03/05/2019 20:21:14|       0.41|      100|           66|\n",
      "|           0.57|03/05/2019 21:21:14|       0.33|      100|           43|\n",
      "|           0.52|03/05/2019 21:46:14|       0.49|      100|           53|\n",
      "|           0.51|03/05/2019 21:56:14|       0.56|      100|           42|\n",
      "|           0.65|03/05/2019 22:01:14|       0.44|      100|           67|\n",
      "|           0.55|03/05/2019 22:11:14|       0.57|      100|           43|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371e1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_sorted = df2_sample.sort('event_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba560e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.54|03/05/2019 08:06:23|       0.38|      105|           74|\n",
      "|           0.64|03/05/2019 08:06:26|       0.55|      107|           56|\n",
      "|           0.78|03/05/2019 08:06:40|       0.22|      115|           62|\n",
      "|           0.65|03/05/2019 08:07:01|       0.36|      128|           83|\n",
      "|           0.63|03/05/2019 08:07:23|       0.39|      139|           93|\n",
      "|           0.41|03/05/2019 08:07:31|       0.45|      143|           37|\n",
      "|           0.39|03/05/2019 08:07:39|       0.63|      147|           49|\n",
      "|           0.74|03/05/2019 08:07:44|       0.27|      149|           66|\n",
      "|           0.58|03/05/2019 08:11:31|       0.61|      110|           76|\n",
      "|           0.62|03/05/2019 08:12:09|       0.63|      132|           59|\n",
      "|           0.52|03/05/2019 08:12:17|       0.44|      136|           54|\n",
      "|           0.54|03/05/2019 08:12:21|       0.45|      138|           39|\n",
      "|           0.73|03/05/2019 08:16:53|       0.46|      123|           81|\n",
      "|           0.95|03/05/2019 08:21:19|       0.24|      103|           83|\n",
      "|           0.26|03/05/2019 08:21:24|       0.45|      106|           38|\n",
      "|           0.46|03/05/2019 08:21:33|       0.27|      111|           74|\n",
      "|            0.7|03/05/2019 08:22:19|       0.18|      137|           89|\n",
      "|           0.61|03/05/2019 08:22:29|       0.36|      142|           67|\n",
      "|           0.35|03/05/2019 08:22:31|       0.34|      143|           66|\n",
      "|           0.62|03/05/2019 08:26:34|       0.39|      112|           83|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e0d2c",
   "metadata": {},
   "source": [
    "**Filter Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17921168",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_path + '/location_temp.csv'\n",
    "df1 = spark.read.format('csv').option('header','true').load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "332ce53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc4047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1['location_id'] == 'loc0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e1fcaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1['location_id'] == 'loc0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "003087ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1['location_id'] == 'loc1').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841b940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc1|          31|\n",
      "|03/04/2019 19:53:06|       loc1|          26|\n",
      "|03/04/2019 19:58:06|       loc1|          31|\n",
      "|03/04/2019 20:03:06|       loc1|          26|\n",
      "|03/04/2019 20:08:06|       loc1|          28|\n",
      "|03/04/2019 20:13:06|       loc1|          27|\n",
      "|03/04/2019 20:18:06|       loc1|          30|\n",
      "|03/04/2019 20:23:06|       loc1|          28|\n",
      "|03/04/2019 20:28:06|       loc1|          28|\n",
      "|03/04/2019 20:33:06|       loc1|          27|\n",
      "|03/04/2019 20:38:06|       loc1|          30|\n",
      "|03/04/2019 20:43:06|       loc1|          32|\n",
      "|03/04/2019 20:48:06|       loc1|          26|\n",
      "|03/04/2019 20:53:06|       loc1|          30|\n",
      "|03/04/2019 20:58:06|       loc1|          26|\n",
      "|03/04/2019 21:03:06|       loc1|          28|\n",
      "|03/04/2019 21:08:06|       loc1|          27|\n",
      "|03/04/2019 21:13:06|       loc1|          28|\n",
      "|03/04/2019 21:18:06|       loc1|          27|\n",
      "|03/04/2019 21:23:06|       loc1|          26|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1['location_id'] == 'loc1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682a46e",
   "metadata": {},
   "source": [
    "**Aggregating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8358d780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|location_id|count|\n",
      "+-----------+-----+\n",
      "|      loc22| 1000|\n",
      "|      loc31| 1000|\n",
      "|      loc82| 1000|\n",
      "|      loc90| 1000|\n",
      "|     loc118| 1000|\n",
      "|      loc39| 1000|\n",
      "|      loc75| 1000|\n",
      "|     loc122| 1000|\n",
      "|      loc24| 1000|\n",
      "|      loc30| 1000|\n",
      "|     loc105| 1000|\n",
      "|      loc96| 1000|\n",
      "|     loc102| 1000|\n",
      "|      loc18| 1000|\n",
      "|      loc27| 1000|\n",
      "|     loc143| 1000|\n",
      "|      loc43| 1000|\n",
      "|     loc123| 1000|\n",
      "|      loc15| 1000|\n",
      "|      loc48| 1000|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('location_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c48a9f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.orderBy('location_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c4abeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|      loc22|           28.251|\n",
      "|      loc31|           25.196|\n",
      "|      loc82|           27.355|\n",
      "|      loc90|           23.216|\n",
      "|     loc118|           24.219|\n",
      "|      loc39|           25.199|\n",
      "|      loc75|           23.209|\n",
      "|     loc122|            32.36|\n",
      "|      loc24|           31.109|\n",
      "|      loc30|           30.211|\n",
      "|     loc105|           26.217|\n",
      "|      loc96|           28.138|\n",
      "|     loc102|           30.327|\n",
      "|      loc18|           30.198|\n",
      "|      loc27|           31.239|\n",
      "|     loc143|           28.213|\n",
      "|      loc43|           32.196|\n",
      "|     loc123|           23.424|\n",
      "|      loc15|           32.171|\n",
      "|      loc48|           30.244|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('location_id').agg({'temp_celcius': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9829efea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|max(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|       loc0|               36|\n",
      "|       loc1|               35|\n",
      "|      loc10|               32|\n",
      "|     loc100|               34|\n",
      "|     loc101|               32|\n",
      "|     loc102|               37|\n",
      "|     loc103|               32|\n",
      "|     loc104|               33|\n",
      "|     loc105|               33|\n",
      "|     loc106|               34|\n",
      "|     loc107|               40|\n",
      "|     loc108|               39|\n",
      "|     loc109|               31|\n",
      "|      loc11|               32|\n",
      "|     loc110|               33|\n",
      "|     loc111|               38|\n",
      "|     loc112|               40|\n",
      "|     loc113|               37|\n",
      "|     loc114|               36|\n",
      "|     loc115|               30|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('location_id').agg({'temp_celcius': 'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5bebfa",
   "metadata": {},
   "source": [
    "**Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da0e5a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab6b99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_s1 = df1.sample(fraction = 0.1, withReplacement = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05ca05ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50146"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_s1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1293d0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|      loc22| 28.36842105263158|\n",
      "|      loc31| 24.85217391304348|\n",
      "|      loc82|27.735632183908045|\n",
      "|      loc90|23.298969072164947|\n",
      "|     loc118|              24.3|\n",
      "|      loc39|25.155555555555555|\n",
      "|      loc75| 23.23076923076923|\n",
      "|     loc122|32.477777777777774|\n",
      "|      loc24|31.703296703296704|\n",
      "|      loc30|             30.27|\n",
      "|     loc105|26.177083333333332|\n",
      "|      loc96|28.254901960784313|\n",
      "|     loc102|30.405405405405407|\n",
      "|      loc18|30.232323232323232|\n",
      "|      loc27| 31.11111111111111|\n",
      "|     loc143| 28.19298245614035|\n",
      "|      loc43|           32.3125|\n",
      "|     loc123|23.591836734693878|\n",
      "|      loc15| 32.22522522522522|\n",
      "|      loc48|             30.11|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_s1.groupBy('location_id').agg({'temp_celcius':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebd4871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|       loc0|29.515151515151516|\n",
      "|       loc1|28.148936170212767|\n",
      "|      loc10|24.956521739130434|\n",
      "|     loc100|             27.08|\n",
      "|     loc101|25.214285714285715|\n",
      "|     loc102|30.405405405405407|\n",
      "|     loc103| 25.60919540229885|\n",
      "|     loc104| 25.89108910891089|\n",
      "|     loc105|26.177083333333332|\n",
      "|     loc106|27.082474226804123|\n",
      "|     loc107|33.655913978494624|\n",
      "|     loc108| 32.20909090909091|\n",
      "|     loc109| 24.27777777777778|\n",
      "|      loc11| 25.02061855670103|\n",
      "|     loc110| 26.51685393258427|\n",
      "|     loc111|31.058823529411764|\n",
      "|     loc112| 33.90291262135922|\n",
      "|     loc113|30.707865168539325|\n",
      "|     loc114|28.897959183673468|\n",
      "|     loc115|23.058823529411764|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_s1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "820f53bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|       loc0|           29.176|\n",
      "|       loc1|           28.246|\n",
      "|      loc10|           25.337|\n",
      "|     loc100|           27.297|\n",
      "|     loc101|           25.317|\n",
      "|     loc102|           30.327|\n",
      "|     loc103|           25.341|\n",
      "|     loc104|           26.204|\n",
      "|     loc105|           26.217|\n",
      "|     loc106|           27.201|\n",
      "|     loc107|           33.268|\n",
      "|     loc108|           32.195|\n",
      "|     loc109|           24.138|\n",
      "|      loc11|           25.308|\n",
      "|     loc110|           26.239|\n",
      "|     loc111|           31.391|\n",
      "|     loc112|           33.359|\n",
      "|     loc113|           30.345|\n",
      "|     loc114|           29.261|\n",
      "|     loc115|           23.239|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754c5dd",
   "metadata": {},
   "source": [
    "**Saving Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05436819",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o102.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 42 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19760/2155774423.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df1.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1238\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m         )\n\u001b[1;32m-> 1240\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m     def orc(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o102.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 42 more\r\n"
     ]
    }
   ],
   "source": [
    "#This is what happens when I try and run the line of code first\n",
    "df1.write.csv('df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32813fc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/C:/Users/ashle/Springboard/Spark SQL/df1.json already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19760/1851949379.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#if I try to run it again this is what it says\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df1.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, path, mode, compression, dateFormat, timestampFormat, lineSep, encoding, ignoreNullFields)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[0mignoreNullFields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignoreNullFields\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[1;32m-> 1095\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m     def parquet(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/C:/Users/ashle/Springboard/Spark SQL/df1.json already exists."
     ]
    }
   ],
   "source": [
    "#if I try to run it again this is what it says\n",
    "df1.write.json('df1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4725463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
